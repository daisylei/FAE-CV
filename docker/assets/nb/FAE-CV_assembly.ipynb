{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAE Common Voice 2022\n",
    "\n",
    "#### Foreign-Accented English from the Common Voice crowdsourced corpus.\n",
    "\n",
    "This project is based on Mozilla's Common-Voice version `cv-corpus-10.0-2022-07-04`.\n",
    "\n",
    "The target criteria are:\n",
    "\n",
    "- Use `validated.tsv` recordings only.\n",
    "- ~20sec per speaker.\n",
    "- 100-500 speakers per class (accent).\n",
    "- Gender balance.\n",
    "- Splits without any speaker overlap.\n",
    "\n",
    "\n",
    "Output:\n",
    "|      File        | Content                                           |\n",
    "|:----------------:|:--------------------------------------------------|\n",
    "|  `config.yaml`   | Parmeters needed to duplicate the assembly.       |\n",
    "|   `train.tsv`    | Split for model training (fine-tuning).           |\n",
    "|     `dev.tsv`    | Split for validation, all in-set labels.          |\n",
    "|    `test.tsv`    | Split for test, all in-set labels                 |\n",
    "|    `eval.tsv`    | plit from left over data, some out-of-set labels. |\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 24 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import tarfile\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import sox\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "MAX_NB_CPU_WORKERS = min(24, int(os.cpu_count() / 4))\n",
    "pandarallel.initialize(\n",
    "    nb_workers=MAX_NB_CPU_WORKERS, use_memory_fs=False, progress_bar=True\n",
    ")\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.create()\n",
    "\n",
    "cfg[\"RANDOM_STATE\"] = 42\n",
    "# INPUT DATA\n",
    "cfg[\"data\"] = dict(\n",
    "    CORPUS_NAME=\"cv-corpus-10.0-2022-07-04\", \n",
    "    LANGUAGE=\"en\", \n",
    "    DIR_CORPORA=\"/corpora\", \n",
    "    DL_HOST='voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com',\n",
    ")\n",
    "\n",
    "# TARGET STATS\n",
    "cfg[\"tgt_stats\"] = dict(\n",
    "    MIN_NR_SPK=65, SAMPLES_PER_SPKR=4, MAX_NUM_SPKRS=500, DUR_CUT_OFF_Q=0.995\n",
    ")\n",
    "# Q=99.5% discards 1% of recordings: 0.5% shortest and 0.5% longest).\n",
    "cfg.data[\n",
    "    \"COMMON_VOICE_URL\"\n",
    "] = f\"https://{cfg.data.DL_HOST}/{cfg.data.CORPUS_NAME}/{cfg.data.CORPUS_NAME}-{cfg.data.LANGUAGE}.tar.gz\"\n",
    "cfg.data[\n",
    "    \"PATH_VALIDATED_TSV\"\n",
    "] = f\"{cfg.data.DIR_CORPORA}/{cfg.data.CORPUS_NAME}/en/validated.tsv\"\n",
    "cfg.data[\n",
    "    \"PATH_MAPPING_TSV\"\n",
    "] = f\"../data/mappings-accents_{cfg.data.CORPUS_NAME}_{cfg.data.LANGUAGE}_v2209.tsv\"\n",
    "cfg.tgt_stats[\"HALF_MAX_NUM_SPKRS\"] = int(cfg.tgt_stats.MAX_NUM_SPKRS / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inits and utils.\n",
    "random.seed(cfg.RANDOM_STATE)\n",
    "def getDuration(\n",
    "    path, base_dir=f\"{cfg.data.DIR_CORPORA}/{cfg.data.CORPUS_NAME}/en/clips/\"\n",
    "):\n",
    "    return sox.file_info.duration(f\"{base_dir}/{path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Download Common-Voice if `validated.tsv` file is not found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing validated.tsv file: /corpora/cv-corpus-10.0-2022-07-04/en/validated.tsv\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(cfg.data.PATH_VALIDATED_TSV):\n",
    "    print(f\"Found existing validated.tsv file: {cfg.data.PATH_VALIDATED_TSV}\")\n",
    "else:\n",
    "    print(f\"Could not find Common Voice, downloading corpus...\")\n",
    "    output_archive_filename = \"../data/cv-en.tar.gz\"\n",
    "    commands = [\n",
    "        \"wget\",\n",
    "        \"--user-agent\",\n",
    "        '\"Mozilla/5.0 (Windows NT 10.0; WOW64) '\n",
    "        'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"',\n",
    "        \"-O\",\n",
    "        output_archive_filename,\n",
    "        f\"{cfg.data.COMMON_VOICE_URL}\",\n",
    "    ]\n",
    "    commands = \" \".join(commands)\n",
    "    subprocess.run(\n",
    "        commands, shell=True, stderr=sys.stderr, stdout=sys.stdout, capture_output=False\n",
    "    )\n",
    "    tar = tarfile.open(output_archive_filename)\n",
    "    try:\n",
    "        tar.extractall(cfg.data.DIR_CORPORA)\n",
    "        print(f\"extracted to: {cfg.data.DIR_CORPORA}\")\n",
    "    except:\n",
    "        print(f\"unable to extract to: {cfg.data.DIR_CORPORA}, trying ../data instead.\")\n",
    "        DIR_CORPORA = \"../data\"\n",
    "        PATH_VALIDATED_TSV = f\"{DIR_CORPORA}/{cfg.data.CORPUS_NAME}/en/validated.tsv\"\n",
    "        tar.extractall(DIR_CORPORA)\n",
    "        print(f\"extracted to: {DIR_CORPORA}\")\n",
    "    tar.close()\n",
    "    os.remove(output_archive_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Load our hand-made mapping table for the accents column, and then load the original common-voice TSV file with \"validated\" entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc2labMapperDF = pd.read_csv(\n",
    "    cfg.data.PATH_MAPPING_TSV, sep=\"\\t\", names=[\"label\", \"accents\"], header=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded orig_corpus.shape:(1589008, 10)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    cfg.data.PATH_VALIDATED_TSV,\n",
    "    sep=\"\\t\",\n",
    "    parse_dates=False,\n",
    "    engine=\"python\",\n",
    "    encoding=\"utf-8\",\n",
    "    on_bad_lines=\"warn\",\n",
    "    quotechar='\"',\n",
    "    quoting=csv.QUOTE_NONE,\n",
    ")\n",
    "print(f\"loaded orig_corpus.shape:{df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4047 unique confirmed-female speakers\n"
     ]
    }
   ],
   "source": [
    "# Add column to indicate if the speaker is a confirmed female or not.\n",
    "isFemaleL = df[df.gender == \"female\"].client_id.unique().tolist()\n",
    "df[\"isfemale\"] = False\n",
    "df.loc[df.client_id.isin(isFemaleL), \"isfemale\"] = True\n",
    "# quick sanity check.\n",
    "assert df[df.isfemale].client_id.nunique() == len(isFemaleL)\n",
    "print(f\"found {len(isFemaleL)} unique confirmed-female speakers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### Start the data clean up by removing unusable lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - removed lines without `accents` value. \t new shape: (841963, 11)\n",
      " - mapped `accents` values to a `label`. \t new shape: (841963, 12)\n",
      " - removed lines without `label` mapping. \t new shape: (833304, 12)\n",
      " - removed lines with `label=-` value. \t new shape: (833194, 12)\n",
      " - removed speakers with less than 4. \t new shape: (828901, 12)\n",
      " ->extracted the residual data with shape: (3708, 12)\n"
     ]
    }
   ],
   "source": [
    "# Start trimming out lines.\n",
    "#\n",
    "df2 = df[df.accents.notnull()]\n",
    "print(f\" - removed lines without `accents` value. \\t new shape: {df2.shape}\")\n",
    "#\n",
    "df3 = pd.merge(left=df2, right=acc2labMapperDF, on=\"accents\", how=\"left\")\n",
    "print(f\" - mapped `accents` values to a `label`. \\t new shape: {df3.shape}\")\n",
    "#\n",
    "df3 = df3[df3.label.notnull()]\n",
    "print(f\" - removed lines without `label` mapping. \\t new shape: {df3.shape}\")\n",
    "#\n",
    "df3 = df3[df3.label != \"-\"]\n",
    "print(f\" - removed lines with `label=-` value. \\t new shape: {df3.shape}\")\n",
    "#\n",
    "# keep some speakers without enough recordings as residualDF (but at least 2).\n",
    "residualDF = df3[\n",
    "    (df3.groupby(\"client_id\").client_id.transform(\"size\")\n",
    "    < cfg.tgt_stats.SAMPLES_PER_SPKR) & \n",
    "    (df3.groupby(\"client_id\").client_id.transform(\"size\")\n",
    "    >= 2)\n",
    "]\n",
    "#\n",
    "df3 = df3[\n",
    "    df3.groupby(\"client_id\").client_id.transform(\"size\")\n",
    "    >= cfg.tgt_stats.SAMPLES_PER_SPKR\n",
    "]\n",
    "print(\n",
    "    f\" - removed speakers with less than {cfg.tgt_stats.SAMPLES_PER_SPKR}. \\t new shape: {df3.shape}\"\n",
    ")\n",
    "print(f\" ->extracted the residual data with shape: {residualDF.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Opportunistically, use the `residualDF` to assemble the `Eval` Set.\n",
    "\n",
    "- Speakers in the `residualDF` above would have been ignored anyway because they do not have enough recordings.\n",
    "- Discard speakers with only 1 recording.\n",
    "- Sample 2 recordings/speaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c0efedab7141b988386636dec569f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=155), Label(value='0 / 155'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add duration info.\n",
    "residualDF['duration'] = residualDF.path.parallel_map(getDuration).values\n",
    "\n",
    "# trim length\n",
    "Q = cfg.tgt_stats.DUR_CUT_OFF_Q\n",
    "dur_cut_off_upper = residualDF.duration.quantile(q=Q)\n",
    "dur_cut_off_lower = residualDF.duration.quantile(q=1-Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keeping only recordings with duration between 10.763160000000005 - 1.868100000000001\n",
      " - residual original shape: (3708, 13)\n",
      " - residual new shape: (3670, 13)\n",
      " - residual data with shape: (3660, 13)\n"
     ]
    }
   ],
   "source": [
    "print(f'keeping only recordings with duration between {dur_cut_off_upper} - {dur_cut_off_lower}')\n",
    "print(f\" - residual original shape: {residualDF.shape}\")\n",
    "residualDF = residualDF[ (residualDF.duration >= dur_cut_off_lower) &  (residualDF.duration <= dur_cut_off_upper) ]\n",
    "print(f\" - residual new shape: {residualDF.shape}\")\n",
    "#\n",
    "# Drop speakers without at least 2 recordings.\n",
    "residualDF = residualDF[\n",
    "    residualDF.groupby(\"client_id\").client_id.transform(\"size\") >= 2\n",
    "]\n",
    "print(f\" - residual data with shape: {residualDF.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table below is what we are working with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label        isfemale\n",
      "africa       True          1\n",
      "australia    False        27\n",
      "             True          6\n",
      "bermuda      False         5\n",
      "             True          2\n",
      "canada       False        54\n",
      "             True          7\n",
      "england      False       166\n",
      "             True         21\n",
      "germany      False         2\n",
      "hispanic     False         1\n",
      "             True          1\n",
      "hongkong     False        12\n",
      "             True          6\n",
      "india        False       282\n",
      "             True         26\n",
      "ireland      False        18\n",
      "             True          1\n",
      "malaysia     False        10\n",
      "             True          3\n",
      "newzealand   False        14\n",
      "             True          3\n",
      "philippines  False         9\n",
      "             True          4\n",
      "scotland     False        14\n",
      "             True          1\n",
      "singapore    False         8\n",
      "sweden       False         1\n",
      "us           False       595\n",
      "             True        112\n",
      "wales        False         4\n",
      "             True          1\n",
      "Name: client_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Selection criteria:\n",
    "# - Two recording per speaker.\n",
    "# - As many speakers/accent-label as available while keeping female/notFemale balanced.\n",
    "#  \n",
    "print(residualDF.groupby(['label','isfemale']).client_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the samples/speaker to only 2.\n",
    "residualDF = pd.DataFrame(\n",
    "    residualDF.groupby(\"client_id\").sample(\n",
    "        n=2,\n",
    "        replace=False,\n",
    "        random_state=cfg.RANDOM_STATE,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvalDF = None\n",
    "for acc in residualDF.label.unique():\n",
    "    #\n",
    "    # Subsample speakers to reach gender balance.\n",
    "    _femDF = residualDF[(residualDF.label == acc) & residualDF.isfemale]\n",
    "    _notFemDF = residualDF[(residualDF.label == acc) & ~residualDF.isfemale]\n",
    "    #\n",
    "    nr_spkrs = min(_femDF.client_id.nunique(), _notFemDF.client_id.nunique())\n",
    "   \n",
    "    _femDF = _femDF.sample(\n",
    "                n=nr_spkrs,\n",
    "                replace=False,\n",
    "                random_state=cfg.RANDOM_STATE,\n",
    "            )\n",
    "    _notFemDF = _notFemDF.sample(\n",
    "                n=nr_spkrs,\n",
    "                replace=False,\n",
    "                random_state=cfg.RANDOM_STATE,\n",
    "            )\n",
    "    if EvalDF is None:\n",
    "        EvalDF = pd.concat([_femDF, _notFemDF])\n",
    "    else:\n",
    "        EvalDF = pd.concat([EvalDF, _femDF, _notFemDF])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table below is what we ende up with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label        isfemale\n",
       "australia    False         6\n",
       "             True          6\n",
       "bermuda      False         2\n",
       "             True          2\n",
       "canada       False         7\n",
       "             True          7\n",
       "england      False        21\n",
       "             True         21\n",
       "hispanic     False         1\n",
       "             True          1\n",
       "hongkong     False         6\n",
       "             True          6\n",
       "india        False        26\n",
       "             True         26\n",
       "ireland      False         1\n",
       "             True          1\n",
       "malaysia     False         3\n",
       "             True          3\n",
       "newzealand   False         3\n",
       "             True          3\n",
       "philippines  False         4\n",
       "             True          4\n",
       "scotland     False         1\n",
       "             True          1\n",
       "us           False       112\n",
       "             True        112\n",
       "wales        False         1\n",
       "             True          1\n",
       "Name: path, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvalDF.groupby(['label','isfemale']).path.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### Find the set of eligible accents with sufficient speaker diversity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "africa            2\n",
       "australia       595\n",
       "bermuda          34\n",
       "canada          795\n",
       "england        1995\n",
       "france            1\n",
       "germany           8\n",
       "hispanic          1\n",
       "holland           3\n",
       "hongkong         99\n",
       "india          1553\n",
       "ireland         160\n",
       "israel            1\n",
       "italy             1\n",
       "malaysia         79\n",
       "newzealand      135\n",
       "norway            1\n",
       "philippines     109\n",
       "poland            4\n",
       "russian           1\n",
       "scotland        141\n",
       "singapore        61\n",
       "thailand          2\n",
       "us             6390\n",
       "wales            59\n",
       "Name: client_id, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.groupby(\"label\").client_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 11 eligible accents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "australia       595\n",
       "canada          795\n",
       "england        1995\n",
       "hongkong         99\n",
       "india          1553\n",
       "ireland         160\n",
       "malaysia         79\n",
       "newzealand      135\n",
       "philippines     109\n",
       "scotland        141\n",
       "us             6390\n",
       "Name: client_id, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eligible accents: at least MIN_NR_SPK.\n",
    "speakersDF = df3.groupby(\"label\").client_id.nunique()\n",
    "speakersDF = speakersDF[speakersDF > cfg.tgt_stats.MIN_NR_SPK]\n",
    "usableAccents = speakersDF.keys().tolist()\n",
    "# nr of speakers per accent label:\n",
    "print(f\"found {len(speakersDF)} eligible accents\")\n",
    "speakersDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - removed lines with unusable accents. \t new shape: (781674, 12)\n"
     ]
    }
   ],
   "source": [
    "df4 = df3[df3.label.isin(usableAccents)]\n",
    "print(f\" - removed lines with unusable accents. \\t new shape: {df4.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Find the time-duration for each audio file and remove outliers (too short/long).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60c9085373041eda3bb50adf09c967f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=32570), Label(value='0 / 32570')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: This may take a couple of minutes (maybe 8mins).\n",
    "# Add duration information column.\n",
    "try:\n",
    "    durs = np.fromfile(open(\"../data/durs.csv\"))\n",
    "    print(\"loaded durs from ../data/durs.csv\")\n",
    "except:\n",
    "    durs = df4.path.parallel_map(getDuration).values\n",
    "    durs.tofile(open(\"../data/durs.csv\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.assign(duration=durs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " // keeping 99.0% of recordings.\n",
      " // dropped recordings <1.68s and >9.756s.\n",
      " - removed lines with too long/short duration. \t new shape: (773907, 13)\n"
     ]
    }
   ],
   "source": [
    "Q = cfg.tgt_stats.DUR_CUT_OFF_Q\n",
    "dur_cut_off_upper = df5.duration.quantile(q=Q)\n",
    "dur_cut_off_lower = df5.duration.quantile(q=1-Q)\n",
    "print(\n",
    "    f\" // keeping {100*(1-2*(1-cfg.tgt_stats.DUR_CUT_OFF_Q))}% of recordings.\\n // dropped recordings <{dur_cut_off_lower}s and >{dur_cut_off_upper}s.\"\n",
    ")\n",
    "df5 = df5[(df5.duration <= dur_cut_off_upper) & (df5.duration >= dur_cut_off_lower)]\n",
    "print(f\" - removed lines with too long/short duration. \\t new shape: {df5.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - removed speakers with less than 4. \t new shape: (773711, 13)\n"
     ]
    }
   ],
   "source": [
    "# One more time discard speakers with less than SAMPLES_PER_SPKR because some may have lost recordings.\n",
    "df5 = df5[\n",
    "    df5.groupby(\"client_id\").client_id.transform(\"size\")\n",
    "    >= cfg.tgt_stats.SAMPLES_PER_SPKR\n",
    "]\n",
    "print(\n",
    "    f\" - removed speakers with less than {cfg.tgt_stats.SAMPLES_PER_SPKR}. \\t new shape: {df5.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Add column indicating if speaker is a confirmed female to guide the following gender balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2391 unique confirmed-female speakers\n"
     ]
    }
   ],
   "source": [
    "isFemaleL = df5[df5.gender == \"female\"].client_id.unique().tolist()\n",
    "df5[\"isfemale\"] = False\n",
    "df5.loc[df5.client_id.isin(isFemaleL), \"isfemale\"] = True\n",
    "print(f\"found {len(isFemaleL)} unique confirmed-female speakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick sanity check.\n",
    "assert df5[df5.isfemale].client_id.nunique() == len(isFemaleL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>accents</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>isfemale</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">australia</th>\n",
       "      <th>False</th>\n",
       "      <td>476</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">canada</th>\n",
       "      <th>False</th>\n",
       "      <td>594</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>198</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">england</th>\n",
       "      <th>False</th>\n",
       "      <td>1657</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>328</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">hongkong</th>\n",
       "      <th>False</th>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">india</th>\n",
       "      <th>False</th>\n",
       "      <td>1355</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>182</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ireland</th>\n",
       "      <th>False</th>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">malaysia</th>\n",
       "      <th>False</th>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">newzealand</th>\n",
       "      <th>False</th>\n",
       "      <td>109</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">philippines</th>\n",
       "      <th>False</th>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">scotland</th>\n",
       "      <th>False</th>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">us</th>\n",
       "      <th>False</th>\n",
       "      <td>4949</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1408</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      client_id  accents\n",
       "label       isfemale                    \n",
       "australia   False           476        5\n",
       "            True            115        1\n",
       "canada      False           594        2\n",
       "            True            198        3\n",
       "england     False          1657       11\n",
       "            True            328        3\n",
       "hongkong    False            71        2\n",
       "            True             28        1\n",
       "india       False          1355        7\n",
       "            True            182        2\n",
       "ireland     False           126        2\n",
       "            True             34        2\n",
       "malaysia    False            56        3\n",
       "            True             23        2\n",
       "newzealand  False           109        2\n",
       "            True             25        2\n",
       "philippines False            81        2\n",
       "            True             28        1\n",
       "scotland    False           118        2\n",
       "            True             22        1\n",
       "us          False          4949       22\n",
       "            True           1408        9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe the current gender-balance per accent label.\n",
    "df5.groupby(by=[\"label\", \"isfemale\"])[[\"client_id\", \"accents\"]].nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Balance by subsampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDF = pd.DataFrame(columns=[df5.columns])\n",
    "for acc in usableAccents:\n",
    "    #\n",
    "    accDF = df5[df5.label == acc]\n",
    "    speakersL = accDF.client_id.unique().tolist()\n",
    "    if len(speakersL) > cfg.tgt_stats.MAX_NUM_SPKRS:\n",
    "        # Too many speakers, let's undersample.\n",
    "        accFemDF = accDF[accDF.isfemale]\n",
    "        accNotFemDF = accDF[~accDF.isfemale]\n",
    "        #\n",
    "        speakersFemL = accFemDF.client_id.unique().tolist()\n",
    "        speakersNotFemL = accNotFemDF.client_id.unique().tolist()\n",
    "        #\n",
    "        assert len(speakersL) == (len(speakersFemL) + len(speakersNotFemL))\n",
    "        #\n",
    "        numFemSpk = len(speakersFemL)\n",
    "        numNotFemSpk = len(speakersNotFemL)\n",
    "        #\n",
    "        # Can afford to subsample notFem speakers?\n",
    "        if numNotFemSpk > cfg.tgt_stats.HALF_MAX_NUM_SPKRS:\n",
    "            # --> Subsample notFem (update lists with subsampled versions).\n",
    "            speakersNotFemL = accNotFemDF.sample(\n",
    "                n=max(\n",
    "                    cfg.tgt_stats.HALF_MAX_NUM_SPKRS,\n",
    "                    cfg.tgt_stats.MAX_NUM_SPKRS - numFemSpk,\n",
    "                ),\n",
    "                replace=False,\n",
    "                random_state=cfg.RANDOM_STATE,\n",
    "            ).client_id.tolist()\n",
    "        #\n",
    "        # Can afford to subsample fem speakers?\n",
    "        if numFemSpk > cfg.tgt_stats.HALF_MAX_NUM_SPKRS:\n",
    "            # --> Subsample Fem (update lists with subsampled versions).\n",
    "            speakersFemL = accFemDF.sample(\n",
    "                n=max(\n",
    "                    cfg.tgt_stats.HALF_MAX_NUM_SPKRS,\n",
    "                    cfg.tgt_stats.MAX_NUM_SPKRS - numNotFemSpk,\n",
    "                ),\n",
    "                replace=False,\n",
    "                random_state=cfg.RANDOM_STATE,\n",
    "            ).client_id.tolist()\n",
    "        #\n",
    "        speakersL = speakersFemL + speakersNotFemL\n",
    "        assert len(speakersL) <= cfg.tgt_stats.MAX_NUM_SPKRS\n",
    "        #\n",
    "    # speaker-trimmed:\n",
    "    accDF2 = accDF[accDF.client_id.isin(speakersL)]\n",
    "    #\n",
    "    # limit the amount of samples per speaker.\n",
    "    accDF3 = pd.DataFrame(\n",
    "        accDF2.groupby(\"client_id\").sample(\n",
    "            n=cfg.tgt_stats.SAMPLES_PER_SPKR,\n",
    "            replace=False,\n",
    "            random_state=cfg.RANDOM_STATE,\n",
    "        )\n",
    "    )\n",
    "    if outDF.empty:\n",
    "        outDF = accDF3\n",
    "    else:\n",
    "        outDF = pd.concat([outDF, accDF3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition into Test, Development and Training sets.\n",
    "\n",
    "- No speaker-overlap\n",
    "- Maintain the gender proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accent    \t E|D|T spkrs \t E|D|T samples\n",
      "-------------------------------------------------\n",
      "australia \t 26|25|201 \t 104|100|804\n",
      "canada \t 34|34|270 \t 136|136|1080\n",
      "england \t 27|27|213 \t 108|108|852\n",
      "hongkong \t 10|10|79 \t 40|40|316\n",
      "india \t 35|35|280 \t 140|140|1120\n",
      "ireland \t 16|16|128 \t 64|64|512\n",
      "malaysia \t 8|8|63 \t 32|32|252\n",
      "newzealand \t 13|14|107 \t 52|56|428\n",
      "philippines \t 11|11|87 \t 44|44|348\n",
      "scotland \t 14|14|112 \t 56|56|448\n",
      "us \t 35|35|278 \t 140|140|1112\n"
     ]
    }
   ],
   "source": [
    "# #####################################################################\n",
    "# partition by separating speakers, keeping male/female balance.\n",
    "#\n",
    "\n",
    "DevDF = pd.DataFrame(columns=[df5.columns])\n",
    "TrainDF = pd.DataFrame(columns=[df5.columns])\n",
    "TestDF = pd.DataFrame(columns=[df5.columns])\n",
    "\n",
    "\n",
    "print(\"accent    \\t E|D|T spkrs \\t E|D|T samples\")\n",
    "print(\"-------------------------------------------------\")\n",
    "for acc in usableAccents:\n",
    "    accDF = outDF[outDF.label == acc]\n",
    "    femSpkrsDF = pd.DataFrame(\n",
    "        accDF[accDF.isfemale].client_id.unique(),\n",
    "        columns=[\"client_id\"],\n",
    "    )\n",
    "    notFemSpkrsDF = pd.DataFrame(\n",
    "        accDF[~accDF.isfemale].client_id.unique(),\n",
    "        columns=[\"client_id\"],\n",
    "    )\n",
    "    testSpeakersDF = pd.concat(\n",
    "        [\n",
    "            femSpkrsDF.sample(frac=0.10, replace=False, random_state=cfg.RANDOM_STATE),\n",
    "            notFemSpkrsDF.sample(\n",
    "                frac=0.10, replace=False, random_state=cfg.RANDOM_STATE\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    accTestDF = accDF[accDF.client_id.isin(testSpeakersDF.client_id)]\n",
    "    #\n",
    "    #\n",
    "    # Continue splitting\n",
    "    # - rm already used speakers.\n",
    "    femSpkrsDF = femSpkrsDF[~femSpkrsDF.client_id.isin(testSpeakersDF.client_id)]\n",
    "    notFemSpkrsDF = notFemSpkrsDF[\n",
    "        ~notFemSpkrsDF.client_id.isin(testSpeakersDF.client_id)\n",
    "    ]\n",
    "    # NOTE: divide by 0.9 to compensate for the missing 10%.\n",
    "    devSpeakersDF = pd.concat(\n",
    "        [\n",
    "            femSpkrsDF.sample(\n",
    "                frac=0.1 / 0.9, replace=False, random_state=cfg.RANDOM_STATE\n",
    "            ),\n",
    "            notFemSpkrsDF.sample(\n",
    "                frac=0.1 / 0.9, replace=False, random_state=cfg.RANDOM_STATE\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    accDevDF = accDF[accDF.client_id.isin(devSpeakersDF.client_id)]\n",
    "    #\n",
    "    #\n",
    "    # Use whatever speakers are left for training.\n",
    "    femSpkrsDF = femSpkrsDF[~femSpkrsDF.client_id.isin(devSpeakersDF.client_id)]\n",
    "    notFemSpkrsDF = notFemSpkrsDF[\n",
    "        ~notFemSpkrsDF.client_id.isin(devSpeakersDF.client_id)\n",
    "    ]\n",
    "    trainSpeakersDF = pd.concat([femSpkrsDF, notFemSpkrsDF])\n",
    "    accTrainDF = accDF[accDF.client_id.isin(trainSpeakersDF.client_id)]\n",
    "    #\n",
    "    #\n",
    "    print(\n",
    "        f\"{acc} \\t {accTestDF.client_id.nunique()}|{accDevDF.client_id.nunique()}|{accTrainDF.client_id.nunique()} \\t {accTestDF.shape[0]}|{accDevDF.shape[0]}|{accTrainDF.shape[0]}\"\n",
    "    )\n",
    "    #\n",
    "    #\n",
    "    # sanity check\n",
    "    for _df in (accTestDF, accDevDF, accTrainDF):\n",
    "        assert _df.client_id.nunique() * cfg.tgt_stats.SAMPLES_PER_SPKR == _df.shape[0]\n",
    "    #\n",
    "    #\n",
    "    # agregate\n",
    "    if TestDF.empty:\n",
    "        TestDF = accTestDF\n",
    "    else:\n",
    "        TestDF = pd.concat([TestDF, accTestDF])\n",
    "    #\n",
    "    if DevDF.empty:\n",
    "        DevDF = accDevDF\n",
    "    else:\n",
    "        DevDF = pd.concat([DevDF, accDevDF])\n",
    "    #\n",
    "    if TrainDF.empty:\n",
    "        TrainDF = accTrainDF\n",
    "    else:\n",
    "        TrainDF = pd.concat([TrainDF, accTrainDF])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Save resulting corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNewLog(cfg):\n",
    "    timeInSeconds = time.time()\n",
    "    timestamp = datetime.fromtimestamp(timeInSeconds).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    directory = f\"../logs/{timestamp}\"\n",
    "    # ensure log folder exists\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    logFile = f\"{directory}/config.yaml\"\n",
    "    OmegaConf.save(config=cfg, f=logFile)\n",
    "    return directory\n",
    "\n",
    "logdir = createNewLog(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvalDF.to_csv(f\"{logdir}/eval.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "TestDF.to_csv(f\"{logdir}/test.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "DevDF.to_csv(f\"{logdir}/dev.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "TrainDF.to_csv(f\"{logdir}/train.tsv\", sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
